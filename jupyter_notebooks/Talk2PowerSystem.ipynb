{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d51186b-568a-4eae-9d2e-83a7084f920b",
   "metadata": {},
   "source": [
    "# Talk2PowerSystem\n",
    "\n",
    "This notebook demonstrates a Natural Language Querying (NLQ) system using [Graphwise GraphDB](https://graphdb.ontotext.com/) and [LangGraph Agents](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0453af1-0311-4fb6-9ef4-0d03eacdf31e",
   "metadata": {},
   "source": [
    "## Read the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc5fdc-e111-46d9-9617-a1371ce8a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"config.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b053819-7b6c-4b41-b47f-aea793906e5c",
   "metadata": {},
   "source": [
    "## Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c563b52f-41f6-44a9-b804-d689d87d2909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logger = logging.getLogger('')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n",
    "\n",
    "logger.handlers.clear()\n",
    "logger.addHandler(handler)\n",
    "\n",
    "logging.getLogger(\"openai\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56682ae-762f-49c6-8e4c-426f6f1a76b1",
   "metadata": {},
   "source": [
    "## Initialize a GraphDB client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbbd7af-c85d-4e46-8235-2864cc250c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttyg.graphdb import GraphDB\n",
    "\n",
    "graph = GraphDB(\n",
    "    base_url=config[\"graphdb\"][\"base_url\"],\n",
    "    repository_id=config[\"graphdb\"][\"repository_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf48eb7-fd2f-410d-a8ef-39cb096d30a0",
   "metadata": {},
   "source": [
    "## Initialize the LLM using Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3df5556-e7e2-4a3e-83d8-8679ad7e7b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "from ttyg.utils import set_env\n",
    "\n",
    "set_env(\"AZURE_OPENAI_API_KEY\")\n",
    "model = AzureChatOpenAI(\n",
    "    azure_endpoint=config[\"llm\"][\"azure_endpoint\"],\n",
    "    api_version=config[\"llm\"][\"api_version\"],\n",
    "    model_name=config[\"llm\"][\"model_name\"],\n",
    "    temperature=config[\"llm\"][\"temperature\"],\n",
    "    seed=config[\"llm\"][\"seed\"],\n",
    "    timeout=config[\"llm\"][\"timeout\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8970f0cb-e50b-4c9f-b1f3-7cc975d969c9",
   "metadata": {},
   "source": [
    "## Define the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01517e71-62b0-46b1-8c01-7c985ce55d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "from ttyg.tools import (\n",
    "    AutocompleteSearchTool,\n",
    "    NowTool,\n",
    "    OntologySchemaAndVocabularyTool,\n",
    "    SparqlQueryTool,\n",
    ")\n",
    "\n",
    "sparql_query_tool = SparqlQueryTool(\n",
    "    graph=graph,\n",
    ")\n",
    "\n",
    "ontology_schema_query = Path(config[\"ontology\"][\"ontology_schema_query_path\"]).read_text()\n",
    "ontology_schema_and_vocabulary_tool = OntologySchemaAndVocabularyTool(\n",
    "    graph=graph,\n",
    "    ontology_schema_query=ontology_schema_query,\n",
    ")\n",
    "\n",
    "string_enumerations_query = Path(config[\"ontology\"][\"string_enumerations_query_path\"]).read_text()\n",
    "results = graph.eval_sparql_query(string_enumerations_query)\n",
    "known_prefixes = graph.get_known_prefixes()\n",
    "sorted_known_prefixes = OrderedDict(sorted(known_prefixes.items(), key=lambda x: len(x[1]), reverse=True))\n",
    "string_enumerations_prompt = \"\"\n",
    "for r in results[\"results\"][\"bindings\"]:\n",
    "    shorten_property = r[\"property\"][\"value\"]\n",
    "    for prefix, namespace in sorted_known_prefixes.items():\n",
    "        if shorten_property.startswith(namespace):\n",
    "            shorten_property = shorten_property.replace(namespace, prefix + \":\")\n",
    "            break\n",
    "    string_enumerations_prompt += f\"\"\"The unique string values of the property {shorten_property} separated with `;` are: {r[\"unique_objects\"][\"value\"]}. \\n\"\"\"\n",
    "\n",
    "autocomplete_search_tool = AutocompleteSearchTool(\n",
    "    graph=graph,\n",
    "    limit=5,\n",
    "    property_path=\"<http://iec.ch/TC57/2013/CIM-schema-cim16#IdentifiedObject.name>\",\n",
    ")\n",
    "\n",
    "now_tool = NowTool()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d1b73-8db2-4c39-bb13-27e6b899e8eb",
   "metadata": {},
   "source": [
    "## Create the ReAct agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1402b7-118c-445e-b670-62cd3df1b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "instructions = f\"\"\"{config['prompts']['assistant_instructions']}\n",
    "\n",
    "The ontology schema to use in SPARQL queries is:\n",
    "\n",
    "```turtle\n",
    "{ontology_schema_and_vocabulary_tool.schema_graph.serialize(format='turtle')}\n",
    "```\n",
    "\n",
    "{string_enumerations_prompt}\n",
    "\"\"\"\n",
    "\n",
    "agent_executor = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[\n",
    "        autocomplete_search_tool,\n",
    "        sparql_query_tool,\n",
    "        now_tool,\n",
    "    ],\n",
    "    state_modifier=instructions,\n",
    "    checkpointer=MemorySaver(),\n",
    "    # debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d75fa-ed06-424a-8a51-cedcba5096f3",
   "metadata": {},
   "source": [
    "## Talk to the power system\n",
    "\n",
    "Note, that at the moment the conversations history is not persisted and is kept in the memory. Upon shut down of the notebook, it will be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53437127-7989-4692-9ceb-fe9542b17d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def print_stream(agent, inputs, config, last_message_id: str = None) -> str:\n",
    "    sum_input_tokens, sum_output_tokens, sum_total_tokens = 0, 0, 0\n",
    "\n",
    "    start = time.time()\n",
    "    for s in agent.stream(inputs, config, stream_mode=\"values\"):\n",
    "        messages = s[\"messages\"]\n",
    "        for message in reversed(messages):\n",
    "            if message.id == last_message_id:\n",
    "                break\n",
    "\n",
    "            message.pretty_print()\n",
    "            if hasattr(message, \"usage_metadata\"):\n",
    "                usage_metadata = message.usage_metadata\n",
    "                input_tokens, output_tokens, total_tokens = usage_metadata[\"input_tokens\"], usage_metadata[\"output_tokens\"], usage_metadata[\"total_tokens\"]\n",
    "                sum_input_tokens += input_tokens\n",
    "                sum_output_tokens += output_tokens\n",
    "                sum_total_tokens += total_tokens\n",
    "                logging.debug(\n",
    "                    f\"Usage: input tokens: {input_tokens}, \"\n",
    "                    f\"output tokens: {output_tokens}, \"\n",
    "                    f\"total tokens: {total_tokens}\")\n",
    "\n",
    "        last_message_id = messages[-1].id\n",
    "\n",
    "    logging.debug(\n",
    "        f\"Total usage: input tokens: {sum_input_tokens}, \"\n",
    "        f\"output tokens: {sum_output_tokens}, \"\n",
    "        f\"total tokens: {sum_total_tokens}\"\n",
    "    )\n",
    "    logging.debug(\n",
    "        f\"Elapsed time: {time.time() - start:.2f} seconds\"\n",
    "    )\n",
    "    return last_message_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec08d41-4493-481c-9e9c-cffd4698a810",
   "metadata": {},
   "source": [
    "### Send consecutive questions in the same thread (conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d9680-9db6-450f-8b08-9854d6b2aa4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf = {\"configurable\": {\"thread_id\": \"thread-123\"}}\n",
    "messages = {\"messages\": [(\"user\", \"List all transformers within Substation OSLO.\")]}\n",
    "last_message_id = print_stream(agent_executor, messages, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f437d74c-4b08-4b60-aea3-8bdf9d59f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = {\"messages\": [(\"user\", \"Give me their descriptions\")]}\n",
    "last_message_id = print_stream(agent_executor, messages, conf, last_message_id=last_message_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2644f3-f8a0-435f-a9ba-04d7186e7e7d",
   "metadata": {},
   "source": [
    "### Or iterate over set of questions, each within a separate thread (conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8373551-3bf0-447d-84cf-cfe340b5e003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"List all transformers within Substation OSLO.\",\n",
    "    \"Liste alle transformatorer innenfor Nettstasjon OSLO.\",\n",
    "    \"List all substations within bidding zone NO2 SGR\",\n",
    "    \"Liste alle nettstasjoner innenfor budsone NO2 SGR\",\n",
    "    \"List all substations that are connected via an AC-line or a DC line to substation named ASKER\",\n",
    "    \"List opp alle understasjoner som er koblet via en AC-linje eller en DC-linje til understasjon kalt ASKER\",\n",
    "    \"List all AC-lines that traverse bidding zones NO5 SGR and NO2 SGR\",\n",
    "    \"List opp alle AC-linjer som krysser budsonene NO5 SGR og NO2 SGR\",\n",
    "    \"Give me 3 measurements in congestion zone NO-ELSP-1\",\n",
    "    \"Gi meg m√•linger i overbelastningssone NO-ELSP-1\",\n",
    "    \"show how many resources are there by class\",\n",
    "    \"List five analogs of type active power\",\n",
    "    \"Give me 8 synchronous machines of type generator\",\n",
    "    \"List transformers that are normally in service\",\n",
    "    \"Give me 5 switches that are normally closed\",\n",
    "    \"List all synchronous machines that have \\\"M1\\\" or \\\"M2\\\" in the name, but not \\\"300\\\"\",\n",
    "    \"Find the PSR f1769e0c\",\n",
    "    \"List all substations north of Trondheim\",\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    conf = {\"configurable\": {\"thread_id\": f\"thread-{i}\"}}\n",
    "    messages = {\"messages\": [(\"user\", question)]}\n",
    "    print_stream(agent_executor, messages, conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
